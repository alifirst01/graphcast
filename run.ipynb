{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-27T06:34:03.685693Z",
     "start_time": "2024-06-27T06:34:03.683019Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import dataclasses\n",
    "import functools\n",
    "import haiku as hk\n",
    "import matplotlib\n",
    "from typing import Optional\n",
    "import jax\n",
    "import numpy as np\n",
    "import xarray\n",
    "import random\n",
    "\n",
    "from graphcast import autoregressive\n",
    "from graphcast import casting\n",
    "from graphcast import checkpoint\n",
    "from graphcast import data_utils\n",
    "from graphcast import graphcast\n",
    "from graphcast import normalization\n",
    "from graphcast import rollout\n",
    "from graphcast import xarray_jax\n",
    "from graphcast import xarray_tree"
   ],
   "id": "5f9541a9a838d2f0",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-27T06:38:09.473697Z",
     "start_time": "2024-06-27T06:38:09.389834Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load existing model params\n",
    "params_file = (\"params_graphCast-ERA5_1979-2017-resolution_\"\n",
    "               \"0.25-pressure_levels_37-mesh_2to6-precipitation_input_and_output.npz\")\n",
    "with open(f\"params/{params_file}\", \"rb\") as f:\n",
    "    ckpt = checkpoint.load(f, graphcast.CheckPoint)\n",
    "\n",
    "params = ckpt.params\n",
    "model_config = ckpt.model_config\n",
    "task_config = ckpt.task_config"
   ],
   "id": "bc209f4d29031154",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-27T06:38:17.049211Z",
     "start_time": "2024-06-27T06:38:17.043835Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Model Parameters\n",
    "GNN_MSG_STEPS = 16\n",
    "HIDDEN_LAYERS = 1\n",
    "LATENT_SIZE = 512\n",
    "MESH2GRID_NORMALIZATION_FACTOR = 0.6180\n",
    "MESH_SIZE = 6\n",
    "RADIUS_QUERY_FRACTION_EDGE_LENGTH = 0.5999\n",
    "RESOLUTION = 0.25\n",
    "\n",
    "# Weather Variables\n",
    "PRESSURE_LEVELS = (1000,)\n",
    "ALL_ATMOSPHERIC_VARS = (\n",
    "    \"potential_vorticity\",\n",
    "    \"specific_rain_water_content\",\n",
    "    \"specific_snow_water_content\",\n",
    "    \"geopotential\",\n",
    "    \"temperature\",\n",
    "    \"u_component_of_wind\",\n",
    "    \"v_component_of_wind\",\n",
    "    \"specific_humidity\",\n",
    "    \"vertical_velocity\",\n",
    "    \"vorticity\",\n",
    "    \"divergence\",\n",
    "    \"relative_humidity\",\n",
    "    \"ozone_mass_mixing_ratio\",\n",
    "    \"specific_cloud_liquid_water_content\",\n",
    "    \"specific_cloud_ice_water_content\",\n",
    "    \"fraction_of_cloud_cover\",\n",
    ")\n",
    "INPUT_SURFACE_VARS = (\n",
    "    \"temperature\",\n",
    "    \"specific_humidity\",\n",
    "    \"total_precipitation_6hr\",\n",
    ")\n",
    "TARGET_SURFACE_VARS = (\n",
    "    \"total_precipitation_6hr\",\n",
    ")\n",
    "TARGET_ATMOSPHERIC_VARS = (\n",
    "    \"specific_humidity\",\n",
    "    \"temperature\",\n",
    ")\n",
    "EXTERNAL_FORCING_VARS = (\n",
    "    \"toa_incident_solar_radiation\",\n",
    ")\n",
    "GENERATED_FORCING_VARS = (\n",
    "    \"year_progress_sin\",\n",
    "    \"year_progress_cos\",\n",
    "    \"day_progress_sin\",\n",
    "    \"day_progress_cos\",\n",
    ")\n",
    "FORCING_VARS = EXTERNAL_FORCING_VARS + GENERATED_FORCING_VARS\n",
    "STATIC_VARS = (\n",
    "    \"geopotential_at_surface\",\n",
    "    \"land_sea_mask\",\n",
    ")\n",
    "\n",
    "# Create a new model and task config\n",
    "params = None\n",
    "state = {}\n",
    "\n",
    "# noinspection PyArgumentList\n",
    "task_config = graphcast.TaskConfig(\n",
    "    input_variables=(\n",
    "            INPUT_SURFACE_VARS + FORCING_VARS + STATIC_VARS),\n",
    "    target_variables=TARGET_SURFACE_VARS + TARGET_ATMOSPHERIC_VARS,\n",
    "    forcing_variables=FORCING_VARS,\n",
    "    pressure_levels=PRESSURE_LEVELS,\n",
    "    input_duration=\"12h\"\n",
    ")\n",
    "\n",
    "# noinspection PyArgumentList\n",
    "model_config = graphcast.ModelConfig(\n",
    "    gnn_msg_steps=GNN_MSG_STEPS,\n",
    "    hidden_layers=HIDDEN_LAYERS,\n",
    "    latent_size=LATENT_SIZE,\n",
    "    mesh2grid_edge_normalization_factor=MESH2GRID_NORMALIZATION_FACTOR,\n",
    "    mesh_size=MESH_SIZE,\n",
    "    radius_query_fraction_edge_length=RADIUS_QUERY_FRACTION_EDGE_LENGTH,\n",
    "    resolution=RESOLUTION,\n",
    ")"
   ],
   "id": "488001088aede524",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "# Load the dataset\n",
    "def parse_file_parts(file_name):\n",
    "    return dict(part.split(\"-\", 1) for part in file_name.split(\"_\"))\n",
    "\n",
    "dataset_file = \"source-era5_date-2022-01-01_res-0.25_levels-37_steps-01.nc\"\n",
    "with open(f\"data/{dataset_file}\", \"rb\") as f:\n",
    "    example_batch = xarray.load_dataset(f).compute()\n",
    "\n",
    "assert example_batch.sizes[\"time\"] >= 3  # 2 for input, >=1 for targets\n",
    "\n",
    "print(\", \".join([f\"{k}: {v}\" for k, v in parse_file_parts(dataset_file.removesuffix(\".nc\")).items()]))"
   ],
   "id": "9ff4d76d6f9165f4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-27T06:39:04.210631Z",
     "start_time": "2024-06-27T06:39:04.169240Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Extract inputs, targets, and forcings\n",
    "train_steps = 1\n",
    "eval_steps = 1\n",
    "\n",
    "train_inputs, train_targets, train_forcings = data_utils.extract_inputs_targets_forcings(\n",
    "    example_batch, target_lead_times=slice(\"6h\", f\"{train_steps * 6}h\"),\n",
    "    **dataclasses.asdict(task_config))\n",
    "\n",
    "eval_inputs, eval_targets, eval_forcings = data_utils.extract_inputs_targets_forcings(\n",
    "    example_batch, target_lead_times=slice(\"6h\", f\"{eval_steps * 6}h\"),\n",
    "    **dataclasses.asdict(task_config))\n",
    "\n",
    "print(\"All Examples:  \", example_batch.dims.mapping)\n",
    "print(\"Train Inputs:  \", train_inputs.dims.mapping)\n",
    "print(\"Train Targets: \", train_targets.dims.mapping)\n",
    "print(\"Train Forcings:\", train_forcings.dims.mapping)\n",
    "print(\"Eval Inputs:   \", eval_inputs.dims.mapping)\n",
    "print(\"Eval Targets:  \", eval_targets.dims.mapping)\n",
    "print(\"Eval Forcings: \", eval_forcings.dims.mapping)"
   ],
   "id": "f6d249dbb433878a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Examples:   {'lon': 1440, 'lat': 721, 'level': 37, 'time': 3, 'batch': 1}\n",
      "Train Inputs:   {'batch': 1, 'time': 2, 'level': 1, 'lat': 721, 'lon': 1440}\n",
      "Train Targets:  {'batch': 1, 'time': 1, 'lat': 721, 'lon': 1440, 'level': 1}\n",
      "Train Forcings: {'batch': 1, 'time': 1, 'lat': 721, 'lon': 1440}\n",
      "Eval Inputs:    {'batch': 1, 'time': 2, 'level': 1, 'lat': 721, 'lon': 1440}\n",
      "Eval Targets:   {'batch': 1, 'time': 1, 'lat': 721, 'lon': 1440, 'level': 1}\n",
      "Eval Forcings:  {'batch': 1, 'time': 1, 'lat': 721, 'lon': 1440}\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-27T06:39:08.901085Z",
     "start_time": "2024-06-27T06:39:08.884601Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load normalization data\n",
    "with open(\"stats/diffs_stddev_by_level.nc\", \"rb\") as f:\n",
    "    diffs_stddev_by_level = xarray.load_dataset(f).compute()\n",
    "with open(\"stats/mean_by_level.nc\", \"rb\") as f:\n",
    "    mean_by_level = xarray.load_dataset(f).compute()\n",
    "with open(\"stats/stddev_by_level.nc\", \"rb\") as f:\n",
    "    stddev_by_level = xarray.load_dataset(f).compute()"
   ],
   "id": "95909daae400db95",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-27T06:39:13.218373Z",
     "start_time": "2024-06-27T06:39:13.209733Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Build jitted functions, and possibly initialize random weights\n",
    "def construct_wrapped_graphcast(\n",
    "        model_config: graphcast.ModelConfig,\n",
    "        task_config: graphcast.TaskConfig):\n",
    "    \"\"\"Constructs and wraps the GraphCast Predictor.\"\"\"\n",
    "    # Deeper one-step predictor.\n",
    "    predictor = graphcast.GraphCast(model_config, task_config)\n",
    "\n",
    "    # Modify inputs/outputs to `graphcast.GraphCast` to handle conversion to\n",
    "    # from/to float32 to/from BFloat16.\n",
    "    predictor = casting.Bfloat16Cast(predictor)\n",
    "\n",
    "    # Modify inputs/outputs to `casting.Bfloat16Cast` so the casting to/from\n",
    "    # BFloat16 happens after applying normalization to the inputs/targets.\n",
    "    predictor = normalization.InputsAndResiduals(\n",
    "        predictor,\n",
    "        diffs_stddev_by_level=diffs_stddev_by_level,\n",
    "        mean_by_level=mean_by_level,\n",
    "        stddev_by_level=stddev_by_level)\n",
    "\n",
    "    # Wraps everything so the one-step model can produce trajectories.\n",
    "    predictor = autoregressive.Predictor(predictor, gradient_checkpointing=True)\n",
    "    return predictor\n",
    "\n",
    "\n",
    "@hk.transform_with_state\n",
    "def run_forward(model_config, task_config, inputs, targets_template, forcings):\n",
    "    predictor = construct_wrapped_graphcast(model_config, task_config)\n",
    "    return predictor(inputs, targets_template=targets_template, forcings=forcings)\n",
    "\n",
    "\n",
    "@hk.transform_with_state\n",
    "def loss_fn(model_config, task_config, inputs, targets, forcings):\n",
    "    predictor = construct_wrapped_graphcast(model_config, task_config)\n",
    "    loss, diagnostics = predictor.loss(inputs, targets, forcings)\n",
    "    return xarray_tree.map_structure(\n",
    "        lambda x: xarray_jax.unwrap_data(x.mean(), require_jax=True),\n",
    "        (loss, diagnostics))\n",
    "\n",
    "\n",
    "def grads_fn(params, state, model_config, task_config, inputs, targets, forcings):\n",
    "    def _aux(params, state, i, t, f):\n",
    "        (loss, diagnostics), next_state = loss_fn.apply(\n",
    "            params, state, jax.random.PRNGKey(0), model_config, task_config,\n",
    "            i, t, f)\n",
    "        return loss, (diagnostics, next_state)\n",
    "\n",
    "    (loss, (diagnostics, next_state)), grads = jax.value_and_grad(\n",
    "        _aux, has_aux=True)(params, state, inputs, targets, forcings)\n",
    "    return loss, diagnostics, next_state, grads\n",
    "\n",
    "\n",
    "# Jax doesn't seem to like passing configs as args through the jit. Passing it\n",
    "# in via partial (instead of capture by closure) forces jax to invalidate the\n",
    "# jit cache if you change configs.\n",
    "def with_configs(fn):\n",
    "    return functools.partial(\n",
    "        fn, model_config=model_config, task_config=task_config)\n",
    "\n",
    "\n",
    "# Always pass params and state, so the usage below are simpler\n",
    "def with_params(fn):\n",
    "    return functools.partial(fn, params=params, state=state)\n",
    "\n",
    "\n",
    "# Our models aren't stateful, so the state is always empty, so just return the\n",
    "# predictions. This is requiredy by our rollout code, and generally simpler.\n",
    "def drop_state(fn):\n",
    "    return lambda **kw: fn(**kw)[0]"
   ],
   "id": "15e8121756cdebbe",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-27T06:39:52.394939Z",
     "start_time": "2024-06-27T06:39:16.555831Z"
    }
   },
   "cell_type": "code",
   "source": [
    "init_jitted = jax.jit(with_configs(run_forward.init))\n",
    "\n",
    "if params is None:\n",
    "    print(\"\\nInitializing the model params...\")\n",
    "    params, state = init_jitted(\n",
    "        rng=jax.random.PRNGKey(0),\n",
    "        inputs=train_inputs,\n",
    "        targets_template=train_targets,\n",
    "        forcings=train_forcings)\n",
    "\n",
    "loss_fn_jitted = drop_state(with_params(jax.jit(with_configs(loss_fn.apply))))\n",
    "grads_fn_jitted = with_params(jax.jit(with_configs(grads_fn)))\n",
    "run_forward_jitted = drop_state(with_params(jax.jit(with_configs(\n",
    "    run_forward.apply))))"
   ],
   "id": "d77db31b4feff37f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initializing the model params...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ali/Desktop/graph-cast/graphcast/autoregressive.py:202: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  scan_length = targets_template.dims['time']\n",
      "WARNING:absl:Skipping gradient checkpointing for sequence length of 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Converting all input data into flat vectors\n",
      "  Transfer data for the grid to the mesh\n",
      "  Run message passing in the multimesh\n",
      "  Transfer data from the mesh to the grid\n",
      "  Convert output flat vectors for the grid nodes to the format of the output\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ali/Desktop/graph-cast/graphcast/autoregressive.py:115: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  num_inputs = inputs.dims['time']\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Train the model\n",
    "# @title Loss computation (autoregressive loss over multiple steps)\n",
    "print(\"\\nTraining the model...\")\n",
    "loss, diagnostics = loss_fn_jitted(\n",
    "    rng=jax.random.PRNGKey(0),\n",
    "    inputs=train_inputs,\n",
    "    targets=train_targets,\n",
    "    forcings=train_forcings)\n",
    "print(\"Loss:\", float(loss))\n",
    "\n",
    "# @title Gradient computation (backprop through time)\n",
    "print(\"\\nGradient computation (backprop through time)...\")\n",
    "loss, diagnostics, next_state, grads = grads_fn_jitted(\n",
    "    inputs=train_inputs,\n",
    "    targets=train_targets,\n",
    "    forcings=train_forcings)\n",
    "mean_grad = np.mean(jax.tree_util.tree_flatten(jax.tree_util.tree_map(lambda x: np.abs(x).mean(), grads))[0])\n",
    "print(f\"Loss: {loss:.4f}, Mean |grad|: {mean_grad:.6f}\")\n",
    "\n",
    "# @title Autoregressive rollout (keep the loop in JAX)\n",
    "print(\"\\nAutoregressive rollout...\")\n",
    "print(\"Inputs:  \", train_inputs.dims.mapping)\n",
    "print(\"Targets: \", train_targets.dims.mapping)\n",
    "print(\"Forcings:\", train_forcings.dims.mapping)\n",
    "\n",
    "predictions = run_forward_jitted(\n",
    "    rng=jax.random.PRNGKey(0),\n",
    "    inputs=train_inputs,\n",
    "    targets_template=train_targets * np.nan,\n",
    "    forcings=train_forcings)"
   ],
   "id": "17862329a4434834"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(\"Inputs:  \", eval_inputs.dims.mapping)\n",
    "print(\"Targets: \", eval_targets.dims.mapping)\n",
    "print(\"Forcings:\", eval_forcings.dims.mapping)\n",
    "\n",
    "predictions = rollout.chunked_prediction(\n",
    "    run_forward_jitted,\n",
    "    rng=jax.random.PRNGKey(0),\n",
    "    inputs=eval_inputs,\n",
    "    targets_template=eval_targets * np.nan,\n",
    "    forcings=eval_forcings)"
   ],
   "id": "45c8e056b569bce1"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-06-27T06:39:52.396211Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def select(\n",
    "    data: xarray.Dataset,\n",
    "    variable: str,\n",
    "    level: Optional[int] = None,\n",
    "    max_steps: Optional[int] = None\n",
    "    ) -> xarray.Dataset:\n",
    "  data = data[variable]\n",
    "  if \"batch\" in data.dims:\n",
    "    data = data.isel(batch=0)\n",
    "  if max_steps is not None and \"time\" in data.sizes and max_steps < data.sizes[\"time\"]:\n",
    "    data = data.isel(time=range(0, max_steps))\n",
    "  if level is not None and \"level\" in data.coords:\n",
    "    data = data.sel(level=level)\n",
    "  return data\n",
    "\n",
    "def scale(\n",
    "    data: xarray.Dataset,\n",
    "    center: Optional[float] = None,\n",
    "    robust: bool = False,\n",
    "    ) -> tuple[xarray.Dataset, matplotlib.colors.Normalize, str]:\n",
    "  vmin = np.nanpercentile(data, (2 if robust else 0))\n",
    "  vmax = np.nanpercentile(data, (98 if robust else 100))\n",
    "  if center is not None:\n",
    "    diff = max(vmax - center, center - vmin)\n",
    "    vmin = center - diff\n",
    "    vmax = center + diff\n",
    "  return (data, matplotlib.colors.Normalize(vmin, vmax),\n",
    "          (\"RdBu_r\" if center is not None else \"viridis\"))\n",
    "\n",
    "plot_pred_variable = \"total_precipitation_6hr\"\n",
    "plot_pred_level = 1000\n",
    "plot_pred_robust = False\n",
    "plot_pred_max_steps = 1\n",
    "plot_max_steps = min(predictions.dims[\"time\"], plot_pred_max_steps.value)\n",
    "\n",
    "data = {\n",
    "    \"Targets\": scale(select(eval_targets, plot_pred_variable, plot_pred_level, plot_max_steps),\n",
    "                     robust=plot_pred_robust),\n",
    "    \"Predictions\": scale(select(predictions, plot_pred_variable, plot_pred_level, plot_max_steps),\n",
    "                         robust=plot_pred_robust),\n",
    "    \"Diff\": scale((select(eval_targets, plot_pred_variable, plot_pred_level, plot_max_steps) -\n",
    "                   select(predictions, plot_pred_variable, plot_pred_level, plot_max_steps)),\n",
    "                  robust=plot_pred_robust, center=0),\n",
    "}\n",
    "\n",
    "# Extract the DataArray from the tuples\n",
    "targets = data[\"Targets\"][0]\n",
    "predictions = data[\"Predictions\"][0]\n",
    "diff = data[\"Diff\"][0]\n",
    "\n",
    "# Number of sets and values per set\n",
    "num_sets = 3\n",
    "values_per_set = 10\n",
    "\n",
    "# Generate random starting indices\n",
    "random_indices = [\n",
    "    (random.randint(0, len(targets.lat) - values_per_set), random.randint(0, len(targets.lon) - values_per_set)) for _\n",
    "    in range(num_sets)]\n",
    "\n",
    "# Initialize the output string\n",
    "fig_title = plot_pred_variable\n",
    "output = fig_title + \"\\n\\n\"\n",
    "\n",
    "# Create header\n",
    "header = f\"{'Lat':>8} {'Lon':>8} {'Target':>10} {'Prediction':>12} {'Diff':>10}\\n\"\n",
    "output += header\n",
    "\n",
    "# Iterate through the random starting indices and print the values\n",
    "for start_i, start_j in random_indices:\n",
    "    output += f\"\\nStarting at index (lat, lon): ({start_i}, {start_j})\\n\"\n",
    "    for k in range(values_per_set):\n",
    "        i = start_i + k\n",
    "        j = start_j + k\n",
    "        if i >= len(targets.lat) or j >= len(targets.lon):\n",
    "            break\n",
    "        lat = float(targets.lat[i])\n",
    "        lon = float(targets.lon[j])\n",
    "        target_value = float(targets[0, i, j])\n",
    "        prediction_value = float(predictions[0, i, j])\n",
    "        diff_value = float(diff[0, i, j])\n",
    "\n",
    "        output += f\"{lat:8.2f} {lon:8.2f} {target_value:10.2f} {prediction_value:12.2f} {diff_value:10.2f}\\n\"\n",
    "\n",
    "output += \"\\n ----------------------------------------------------- \\n\"\n",
    "print(output)\n",
    "# Save to a README file\n",
    "with open(\"/predictions.txt\", \"a\") as file:\n",
    "    file.write(output)\n",
    "\n",
    "print(\"Output saved to predictions.txt\")"
   ],
   "id": "78d06e1629a9b2e9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:   {'batch': 1, 'time': 2, 'level': 1, 'lat': 721, 'lon': 1440}\n",
      "Targets:  {'batch': 1, 'time': 1, 'lat': 721, 'lon': 1440, 'level': 1}\n",
      "Forcings: {'batch': 1, 'time': 1, 'lat': 721, 'lon': 1440}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ali/Desktop/graph-cast/graphcast/rollout.py:127: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  num_target_steps = targets_template.dims[\"time\"]\n",
      "/Users/ali/Desktop/graph-cast/graphcast/autoregressive.py:202: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  scan_length = targets_template.dims['time']\n",
      "WARNING:absl:Skipping gradient checkpointing for sequence length of 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Converting all input data into flat vectors\n",
      "  Transfer data for the grid to the mesh\n",
      "  Run message passing in the multimesh\n",
      "  Transfer data from the mesh to the grid\n",
      "  Convert output flat vectors for the grid nodes to the format of the output\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ali/Desktop/graph-cast/graphcast/autoregressive.py:115: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  num_inputs = inputs.dims['time']\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "1855304ba8bdf218"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
